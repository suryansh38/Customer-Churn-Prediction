# -*- coding: utf-8 -*-
"""Churn_prediction<BA>.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XV5YVq2zfyBQ9N14vIxMzGSY6yt582rX
"""

!unzip /content/telco-customer-churn.zip

"""##LOADING LIBRARIES"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px

data = pd.read_csv("/content/WA_Fn-UseC_-Telco-Customer-Churn.csv") ##Reading data

data.head(5)        ##first 5 rows of dataset

data.shape  ##shape of dataset

"""comment: Dataset contains 7043 unique customer id with 20 attributes."""

data.dtypes          ##attribute's datatype

"""It is interesting to see that the TotalCharges columns is object datatype but actually it should be float datatype."""

data.isnull().mean()             ##Checking null values in every column of dataset

"""It seems that there is no any null values in dataset but wait...."""

(data["TotalCharges"]==" ").sum()

"""*TotalCharges* columns contains some blank values.

Assumptions:
1. It may be the case that customer have not paid any total amount or charges may be waived out by the company.
2. These gap may be left accidently.

for now we will assume first case and fill those blank gap with 0.
"""

data['TotalCharges'] = data["TotalCharges"].replace(" ",0)

(data['TotalCharges']==" ").sum()       ##just confirming

data["TotalCharges"] = data["TotalCharges"].astype(float)      ##Changing the datatype into float

data.describe()            ##Statical description of numerical feature

"""It may be the case that these variables contains some outliers. For checking out those outlier we will plot BOX_PLOT.

##UNIVARIATE ANALYSIS
"""

import plotly.graph_objects as go
fig = go.Figure()

fig.add_trace(go.Box(y=data["MonthlyCharges"], name= "MonthlyCharges"))
fig.add_trace(go.Box(y=data["TotalCharges"]/100, name="TotalCharges"))  ##scaling is done to make its value comparable to monthly charges and tenure
fig.add_trace(go.Box(y=data["tenure"], name="Tenure"))

fig.show()

"""


* from all the plot we can conclude that columns: TotalCharges, Montlycharges and tenure do not contains any outliers



"""

from scipy import stats
from scipy.stats import norm, skew
import seaborn as sns
color = sns.color_palette()

sns.distplot(data['TotalCharges'] ,fit=norm);

sns.distplot(data['MonthlyCharges'] ,fit=norm);

sns.distplot(data['tenure'] ,fit=norm);

"""From the above plot we can conclude that 

* Skewness of distribution of TotalCharges, MonthlyCharges and tenure are right skewed .

* Kurtosis of distribution of TotalCharges, MonthlyCharges and tenure are of Leptokurtic in nature.



"""

def kdeplot(feature):
    plt.figure(figsize=(9, 4))
    plt.title("KDE for {}".format(feature))
    ax0 = sns.kdeplot(data[data['Churn'] == 'No'][feature].dropna(), color= 'navy', label= 'Churn: No')
    ax1 = sns.kdeplot(data[data['Churn'] == 'Yes'][feature].dropna(), color= 'orange', label= 'Churn: Yes')

kdeplot('tenure')
kdeplot('MonthlyCharges')
kdeplot('TotalCharges')

"""Plotting above feature's distribution on dividing them on the basis of churn category and not churn category we conclude that:

1. Customers with small tenure are more likely to churn
2. Customers having low monthly charge is less likely to churn
3. Customers having high TotalCharges are more likely to churn.
"""

import warnings
warnings.filterwarnings("ignore")

data[["MonthlyCharges", "TotalCharges", "tenure"]].hist()

"""Conclusion

1. Customers with low TotalCharges, MonthlyCharges and Tenure are in majority.

2. Count of customer decrease approximately exponentially 
as Total Charges increases. 

3. Count of customers with intermediate tenure is almost constant.
"""

sns.countplot(x="Churn", hue="Churn", data=data)

data["Churn"].value_counts(normalize=True)



"""#BIVARIATE ANALYSIS"""

import plotly.graph_objects as go
from plotly.subplots import make_subplots

churn = data[data["Churn"] == "Yes"]
not_churn = data[data["Churn"] == "No"]

def pie_plot(columns):
  lab2 =not_churn[columns].value_counts().keys().tolist()
  val2 = not_churn[columns].value_counts().values.tolist()
  lab1 =churn[columns].value_counts().keys().tolist()
  val1 =churn[columns].value_counts().values.tolist()
  fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])
  fig.add_trace(go.Pie(labels=lab1, values=val1, name="Churn"),
              1, 1)
  fig.add_trace(go.Pie(labels=lab2, values=val2, name="Not churn"),
              1, 2)

  # Use `hole` to create a donut-like pie chart
  fig.update_traces(hole=.4, hoverinfo="label+percent+name")

  fig.update_layout(
    title_text=str(columns)+" vs churn",
    # Add annotations in the center of the donut pies.
    annotations=[dict(text='Churn', x=0.18, y=0.5, font_size=20, showarrow=False),
                 dict(text='Not churn', x=0.85, y=0.5, font_size=20, showarrow=False)])
  fig.show()

cat_features = ["gender","Partner","Dependents","PhoneService","MultipleLines","InternetService","OnlineSecurity","OnlineBackup","DeviceProtection","TechSupport","StreamingTV","StreamingMovies","Contract","PaperlessBilling","PaymentMethod"]

for x in cat_features:
  pie_plot(x)

"""#Categorical feature encoding"""

cat_features = ["gender","Partner","Dependents","PhoneService","MultipleLines","InternetService","OnlineSecurity","OnlineBackup","DeviceProtection","TechSupport","StreamingTV","StreamingMovies","Contract","PaperlessBilling","PaymentMethod","Churn"]

from sklearn.preprocessing import LabelEncoder

encoder=LabelEncoder()

encoded = data[cat_features].apply(encoder.fit_transform)

encoded.head()

data = data.drop(columns=cat_features)

data = pd.concat([data,encoded],axis=1)

data.head()

data = data.drop(columns=["customerID"])

data.head()

col_name=[]
corr=[]
for col in data.columns:
  if(col!="Churn"):
    col_name.append(col)
    corr.append(data[col].corr(data["Churn"]))

correlation = pd.DataFrame()
correlation["Attributes"]=col_name
correlation["Correlation_with_target_feature"]=corr

correlation.head(25)

import plotly.graph_objects as go

fig = go.Figure(go.Bar(
            x=corr,
            y=col_name,
            orientation='h'))

fig.show()

"""#MULTIVARIATE ANALYSIS"""

corr = data.corr()
ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True
)
ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
);

"""#MODEL SELECTION"""

y = data.pop("Churn")

X =  data

from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics

X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.3, random_state=42, stratify = y)

X_train.head()

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test =  scaler.transform(X_test)

X_train = pd.DataFrame(X_train, columns=data.columns)
X_test = pd.DataFrame(X_test, columns=data.columns)

"""#Decision Tree"""

model1 = DecisionTreeClassifier()          #decision tree classifier
model1 = model1.fit(X_train,y_train)
y_pred = model1.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

from sklearn.metrics import confusion_matrix, f1_score, plot_confusion_matrix
from sklearn.metrics import classification_report

# Thanks to https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud
def plot_cm(y_true, y_pred, title):
    figsize=(6,6)
    y_pred = y_pred.astype(int)
    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))
    cm_sum = np.sum(cm, axis=1, keepdims=True)
    cm_perc = cm / cm_sum.astype(float) * 100
    annot = np.empty_like(cm).astype(str)
    nrows, ncols = cm.shape
    for i in range(nrows):
        for j in range(ncols):
            c = cm[i, j]
            p = cm_perc[i, j]
            if i == j:
                s = cm_sum[i]
                annot[i, j] = '%.1f%%\n%d/%d' % (p, c, s)
            elif c == 0:
                annot[i, j] = ''
            else:
                annot[i, j] = '%.1f%%\n%d' % (p, c)
    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))
    cm.index.name = 'Actual'
    cm.columns.name = 'Predicted'
    fig, ax = plt.subplots(figsize=figsize)
    plt.title(title)
    sns.heatmap(cm, cmap='viridis', annot=annot, fmt='', ax=ax)

plot_cm(y_test, y_pred, 'Only Decision_Tree \n f1=' + str('%.4f' %metrics.accuracy_score(y_test, y_pred)))

print(metrics.accuracy_score(y_test, y_pred))
print(classification_report(y_test,y_pred))

"""we can see that precision and recall of class 1 is very low

#Random Forest
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

forest = RandomForestClassifier(n_estimators=100, random_state=17, n_jobs=4)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# forest.fit(X_train, y_train)

pred2 = forest.predict(X_test)

plot_cm(y_test, pred2, 'Only Random_forest \n f1=' + str('%.4f' %accuracy_score(y_test, pred2)))

print(accuracy_score(y_test, pred2))
print(classification_report(y_test,pred2))

"""precision for class 1 improve."""



"""

#Logistic Regression"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

logit = LogisticRegression(C=1, solver='lbfgs', max_iter=500,
                           random_state=17, n_jobs=4)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# logit.fit(X_train, y_train)

pred = logit.predict(X_test)

print(accuracy_score(y_test, pred))
print(classification_report(y_test,pred))

plot_cm(y_test, y_pred, 'Logistic_Regression \n f1=' + str('%.4f' %accuracy_score(y_test, pred)))

"""Precision as well as Recall improves with logistic regression."""

##Lets try some advance model

"""#Light Gbm"""

from lightgbm import LGBMClassifier

lgb_clf = LGBMClassifier(random_state=17)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# lgb_clf.fit(X_train, y_train)

accuracy_score(y_test, lgb_clf.predict(X_test))

"""Lets do hyperparameter tunning

stage 1 of hyperparameter tunning
"""

param_grid = {'num_leaves': [7, 15, 31, 63], 
              'max_depth': [3, 4, 5, 6, -1]}

from sklearn.model_selection import GridSearchCV

grid_searcher = GridSearchCV(estimator=lgb_clf, param_grid=param_grid, 
                             cv=5, verbose=1, n_jobs=4)

grid_searcher.fit(X_train, y_train)

grid_searcher.best_params_, grid_searcher.best_score_

accuracy_score(y_test, grid_searcher.predict(X_test))

"""stage 2 of hyperparameter tunning

learning rate tunning
"""

num_iterations = 200
lgb_clf2 = LGBMClassifier(random_state=17, max_depth=3, 
                          num_leaves=15, n_estimators=num_iterations,
                          n_jobs=1)

param_grid2 = {'learning_rate': np.logspace(-3, 0, 10)}
grid_searcher2 = GridSearchCV(estimator=lgb_clf2, param_grid=param_grid2,
                               cv=5, verbose=1, n_jobs=4)
grid_searcher2.fit(X_train, y_train)
print(grid_searcher2.best_params_, grid_searcher2.best_score_)
print(accuracy_score(y_test, grid_searcher2.predict(X_test)))

final_lgb = LGBMClassifier(n_estimators=500, num_leaves=15,
                           learning_rate=0.02154, max_depth=3,
                         n_jobs=4)

final_lgb.fit(X_train,y_train)

lgb_final_pred = final_lgb.predict(X_test)

accuracy_score(y_test,lgb_final_pred)

print(accuracy_score(y_test, lgb_final_pred))
print(classification_report(y_test,lgb_final_pred))

"""Precision of majority class have increased
but it is showing poor performance for minority class
"""

plot_cm(y_test, y_pred, 'Logistic_Regression with oversampling \n f1=' + str('%.4f' %accuracy_score(y_test, pred)))

"""#Further Improvement

1. Feature engineering
2. Oversampling
3. Using class weight
4. Feature Selection

Since TotalCharges MonthlyCharges and tenure are some best feature we have, we can generate some new feature from it. but it does not improve accuracy so for now we are neglecting this one.
"""

#data["discount"] = (data["MonthlyCharges"]-(data["TotalCharges"]/(data["tenure"]+1)))##Let's name this feature discount

#X_train, X_test, y_train, y_test = train_test_split(data.values, y, test_size=0.3, random_state=42, stratify = y)

"""Till now the best model we have is Logistic regression we will select it for further experiment

Now we will do upsampling to increase the amount of minority class data
"""

from imblearn.over_sampling import SMOTE

sm = SMOTE(random_state=42)        ##Oversampling
X_res, y_res = sm.fit_resample(X_train, y_train)

distri = pd.DataFrame()
distri["Condition"] = ["Before_Oversampling","After_Oversampling"]

distri["Count_of_Minority_class"] = [(y_train==1).sum(),(y_res==1).sum()]
distri["Count_of_Majority_class"] = [(y_train==0).sum(),(y_res==0).sum()]

distri.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# logit.fit(X_res, y_res)

pred = logit.predict(X_test)

print(accuracy_score(y_test, pred))
print(classification_report(y_test,pred))

plot_cm(y_test, pred, 'Logistic_Regression with oversampling \n f1=' + str('%.4f' %accuracy_score(y_test, pred)))

"""Although Recall of minority classes has improved a lot but overall accuracy reduced. Precision of Minority class is still low

Another approach we will apply class weight while prediction.
For this we will use light gbm model

#Applying_Class Weight
"""

def get_class_weight(classes, exp=1):
    '''
    Weight of the class is inversely proportional to the population of the class.
    There is an exponent for adding more weight.
    '''
    hist, _ = np.histogram(classes, bins=np.arange(3)-0.5)
    class_weight = hist.sum()/np.power(hist, exp)
    
    return class_weight

class_weight = get_class_weight(y_train)
print('class_weight=', class_weight)
plt.figure()
plt.title('classes')
plt.hist(y_train, bins=np.arange(3)-0.5)
plt.figure()
plt.title('class_weight')
plt.bar(np.arange(2), class_weight)
plt.title('class_weight')

import lightgbm as lgb

dataset = lgb.Dataset(X_train, label=y_train, weight=class_weight[y_train])
params = {'learning_rate': 0.1,
          'num_class': 1,
          'metric': 'logloss',
          'verbose': 1,
          'learning_rate':0.021}

# fit the model
print('Training LGBM...')
gbc = lgb.train(params, dataset, 500, verbose_eval=10)
print('LGBM trained!')

gbc_pred = gbc.predict(X_test)
print('gbc_pred.shape=', gbc_pred.shape)

pred = np.round(gbc_pred)

accuracy_score(pred,y_test)

print(classification_report(y_test,pred))

"""Precision of minority class improve a lot.

#Feature Selection
"""

from numpy import sort
from sklearn.feature_selection import SelectFromModel
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

logit = LogisticRegression(C=1, solver='lbfgs', max_iter=500,
                           random_state=17, n_jobs=4)

acc=[]
maximum_acc=0
for x in range(1,20):
  sfs = SFS(logit,k_features=x,forward=True,floating=False,scoring = 'accuracy',cv = 0)
  sfs.fit(X_train, y_train)
  col_n = list((sfs.k_feature_names_))
  logit.fit(X_train[col_n],y_train)
  ac = accuracy_score(y_test,logit.predict(X_test[col_n]))
  if(ac>maximum_acc):
    maximum_acc=ac
    best_feature = col_n
  acc.append(ac)
  print("Accuracy after selection of",x,"best feature is: ",accuracy_score(y_test,logit.predict(X_test[col_n])))

X = list(range(1,20))

import plotly.graph_objects as go
import numpy as np

fig = go.Figure(data=go.Scatter(x=X, y=acc))
fig.update_layout(title='Count of best features vs accuracy',
                   xaxis_title='Count of features',
                   yaxis_title='Accuracy')
fig.show()

best_feature

logit.fit(X_train[best_feature],y_train)
ac = accuracy_score(y_test,logit.predict(X_test[best_feature]))

print(ac)
print(classification_report(y_test,logit.predict(X_test[best_feature])))

"""Best till now...."""

